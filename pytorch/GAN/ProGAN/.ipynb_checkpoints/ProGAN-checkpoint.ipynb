{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4ea1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f9e3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2D(nn.Module):\n",
    "    def __init__(self,in_filters,out_filters,kernel_size=3,stride=1,padding=1,gain=2):\n",
    "        super(WSConv2D,self).__init__()\n",
    "        self.conv = nn.Conv2d(in_filters,out_filters,kernel_size,stride,padding)\n",
    "        self.scale = (gain/(in_filters*kernel_size ** 2)) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "        \n",
    "        #intialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a4e91e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm,self).__init__()\n",
    "        self.eplison = 1e-8\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x / torch.sqrt(torch.mean(x**2,dim=1,keepdim=True) + self.eplison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad33238",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, use_pixelnorm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.use_pn = use_pixelnorm\n",
    "        self.conv1 = WSConv2D(in_filters, out_filters)\n",
    "        self.conv2 = WSConv2D(out_filters, out_filters)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc2d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [1,1,1,1,1/2,1/4,1/8,1/16,1/32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa6c825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_space, in_filters, img_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # initial takes 1x1 -> 4x4\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(latent_space, in_filters, 4, 1, 0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2D(in_filters, in_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(),\n",
    "        )\n",
    "\n",
    "        self.initial_rgb = WSConv2D(\n",
    "            in_filters, img_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.prog_blocks, self.rgb_layers = (\n",
    "            nn.ModuleList([]),\n",
    "            nn.ModuleList([self.initial_rgb]),\n",
    "        )\n",
    "\n",
    "        for i in range(\n",
    "            len(factors) - 1\n",
    "        ):  # -1 to prevent index error because of factors[i+1]\n",
    "            conv_in_c = int(in_filters * factors[i])\n",
    "            conv_out_c = int(in_filters * factors[i + 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2D(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        out = self.initial(x)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out)\n",
    "\n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "\n",
    "        # The number of channels in upscale will stay the same, while\n",
    "        # out which has moved through prog_blocks might change. To ensure\n",
    "        # we can convert both to rgb we use different rgb_layers\n",
    "        # (steps-1) and steps for upscaled, out respectively\n",
    "        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "add80813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,in_filters, img_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        # here we work back ways from factors because the discriminator\n",
    "        # should be mirrored from the generator. So the first prog_block and\n",
    "        # rgb layer we append will work for input size 1024x1024, then 512->256-> etc\n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "            conv_in = int(in_filters * factors[i])\n",
    "            conv_out = int(in_filters * factors[i - 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in, conv_out, use_pixelnorm=False))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2D(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "        # perhaps confusing name \"initial_rgb\" this is just the RGB layer for 4x4 input size\n",
    "        # did this to \"mirror\" the generator initial_rgb\n",
    "        self.initial_rgb = WSConv2D(\n",
    "            img_channels, in_filters, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(\n",
    "            kernel_size=2, stride=2\n",
    "        )  # down sampling using avg pool\n",
    "\n",
    "        # this is the block for 4x4 input size\n",
    "        self.final_block = nn.Sequential(\n",
    "            # +1 to in_filters because we concatenate from MiniBatch std\n",
    "            WSConv2D(in_filters + 1, in_filters, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2D(in_filters, in_filters, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2D(\n",
    "                in_filters, 1, kernel_size=1, padding=0, stride=1\n",
    "            ),  # we use this instead of linear layer\n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        \"\"\"Used to fade in downscaled using avg pooling and output from CNN\"\"\"\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        batch_statistics = (\n",
    "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        )\n",
    "        # we take the std for each example (across all channels, and pixels) then we repeat it\n",
    "        # for a single channel and concatenate it with the image. In this way the discriminator\n",
    "        # will get information about the variation in the batch/image\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        # where we should start in the list of prog_blocks, maybe a bit confusing but\n",
    "        # the last is for the 4x4. So example let's say steps=1, then we should start\n",
    "        # at the second to last because input_size will be 8x8. If steps==0 we just\n",
    "        # use the final block\n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "\n",
    "        # convert from rgb as initial step, this will depend on\n",
    "        # the image size (each will have it's on rgb layer)\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "        if steps == 0:  # i.e, image is 4x4\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        # because prog_blocks might change the channels, for down scale we use rgb_layer\n",
    "        # from previous/smaller size which in our case correlates to +1 in the indexing\n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "\n",
    "        # the fade_in is done first between the downscaled and the input\n",
    "        # this is opposite from the generator\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55dc39f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_noise = 100\n",
    "# in_filters = 256\n",
    "# gen = Generator(latent_noise, in_filters, img_channels=3)\n",
    "# critic = Discriminator(in_filters, img_channels=3)\n",
    "\n",
    "# for img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "#     num_steps = int(log2(img_size / 4))\n",
    "#     x = torch.randn((1, latent_noise, 1, 1))\n",
    "#     z = gen(x, 0.5, num_steps)\n",
    "#     assert z.shape == (1, 3, img_size, img_size)\n",
    "#     out = critic(z, alpha=0.5, steps=num_steps)\n",
    "#     assert out.shape == (1, 1)\n",
    "#     print(f\"Success! At img size: {img_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9e685e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TRAIN_AT_IMG_SIZE = 128\n",
    "DATASET = 'dataset/train'\n",
    "CHECKPOINT_GEN = \"generator.pth\"\n",
    "CHECKPOINT_CRITIC = \"critic.pth\"\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = False\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZES = [32, 32, 32, 16, 16, 16, 16, 8, 4]\n",
    "latent_space = 256  # should be 512 in original paper\n",
    "in_filters = 256  # should be 512 in original paper\n",
    "CRITIC_ITERATIONS = 1\n",
    "LAMBDA_GP = 10\n",
    "PROGRESSIVE_EPOCHS = [10] * len(BATCH_SIZES)\n",
    "FIXED_NOISE = torch.randn(8, latent_space, 1, 1)\n",
    "NUM_WORKERS = 4\n",
    "img_channels=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2ec7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def plot_to_tensorboard(\n",
    "    writer, loss_critic, loss_gen, real, fake, tensorboard_step\n",
    "):\n",
    "    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # take out (up to) 8 examples to plot\n",
    "        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n",
    "        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n",
    "        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "\n",
    "def generate_examples(gen, steps, truncation=0.7, n=100):\n",
    "    \"\"\"\n",
    "    Tried using truncation trick here but not sure it actually helped anything, you can\n",
    "    remove it if you like and just sample from torch.randn\n",
    "    \"\"\"\n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    for i in range(n):\n",
    "        noise = torch.tensor(truncnorm.rvs(-truncation, truncation, size=(1, config.Z_DIM, 1, 1)), device=config.DEVICE, dtype=torch.float32)\n",
    "        img = gen(noise, alpha, steps)\n",
    "        save_image(img*0.5+0.5, f\"saved_examples/img_{i}.png\")\n",
    "    gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d40324be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 21:25:05.471740: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image size: 128\n",
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/450 [00:27<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors does not require grad",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 181\u001b[0m\n\u001b[1;32m    176\u001b[0m                 save_checkpoint(critic, opt_critic, filename\u001b[38;5;241m=\u001b[39mCHECKPOINT_CRITIC)\n\u001b[1;32m    178\u001b[0m         step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# progress to the next img size\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 161\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m     tensorboard_step, alpha \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt_critic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SAVE_MODEL:\n\u001b[1;32m    175\u001b[0m         save_checkpoint(gen, opt_gen, filename\u001b[38;5;241m=\u001b[39mCHECKPOINT_GEN)\n",
      "Cell \u001b[0;32mIn[11], line 77\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(critic, gen, loader, dataset, step, alpha, opt_critic, opt_gen, tensorboard_step, writer)\u001b[0m\n\u001b[1;32m     75\u001b[0m critic_image \u001b[38;5;241m=\u001b[39m critic(image, alpha, step)\n\u001b[1;32m     76\u001b[0m critic_fake_image \u001b[38;5;241m=\u001b[39m critic(fake_image\u001b[38;5;241m.\u001b[39mdetach(), alpha, step)\n\u001b[0;32m---> 77\u001b[0m gp \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_penalty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m loss_critic \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;241m-\u001b[39m(torch\u001b[38;5;241m.\u001b[39mmean(critic_image) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(critic_fake_image))\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;241m+\u001b[39m LAMBDA_GP \u001b[38;5;241m*\u001b[39m gp\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(critic_image \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     84\u001b[0m opt_critic\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[11], line 41\u001b[0m, in \u001b[0;36mgradient_penalty\u001b[0;34m(critic, image, fake_image, alpha, train_step)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#calculate the critic scores\u001b[39;00m\n\u001b[1;32m     39\u001b[0m mixed_scores \u001b[38;5;241m=\u001b[39m critic(interpolated_images, alpha, train_step)\n\u001b[0;32m---> 41\u001b[0m gradient \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minterpolated_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmixed_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixed_scores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     49\u001b[0m gradient \u001b[38;5;241m=\u001b[39m gradient\u001b[38;5;241m.\u001b[39mview(gradient\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     50\u001b[0m gradient_norm \u001b[38;5;241m=\u001b[39m gradient\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;241m2\u001b[39m,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/joker/.joker/lib/python3.10/site-packages/torch/autograd/__init__.py:319\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    317\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    323\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(output \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28minput\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    324\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m (output, \u001b[38;5;28minput\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, t_inputs))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors does not require grad"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_loader(image_size):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.Normalize(\n",
    "                [0.5 for _ in range(img_channels)],\n",
    "                [0.5 for _ in range(img_channels)],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    batch_size = BATCH_SIZES[int(log2(image_size / 4))]\n",
    "    dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return loader, dataset\n",
    "\n",
    "def gradient_penalty(critic,image,fake_image,alpha,train_step):\n",
    "    batch_size, C, H, W = image.shape\n",
    "    eplison = torch.rand((batch_size, 1, 1, 1)).repeat(1, C, H, W)\n",
    "    interpolated_images = image * eplison + fake_image.detach() * (1 - eplison)\n",
    "   \n",
    "    #calculate the critic scores\n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
    "    \n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = interpolated_images,\n",
    "        outputs = mixed_scores,\n",
    "        grad_outputs = torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    gradient = gradient.view(gradient.shape[0],-1)\n",
    "    gradient_norm = gradient.norm(2,dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm-1)**2)\n",
    "    return gradient_penalty\n",
    "\n",
    "def train_fn(\n",
    "    critic,\n",
    "    gen,\n",
    "    loader,\n",
    "    dataset,\n",
    "    step,\n",
    "    alpha,\n",
    "    opt_critic,\n",
    "    opt_gen,\n",
    "    tensorboard_step,\n",
    "    writer\n",
    "):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch_idx, (image, _) in enumerate(loop):\n",
    "        cur_batch_size = image.shape[0]\n",
    "\n",
    "        # Train Critic: max E[critic(image)] - E[critic(fake_image)] <-> min -E[critic(image)] + E[critic(fake_image)]\n",
    "        # which is equivalent to minimizing the negative of the expression\n",
    "        noise = torch.randn(cur_batch_size, latent_space, 1, 1)\n",
    "\n",
    "        fake_image = gen(noise, alpha, step)\n",
    "        critic_image = critic(image, alpha, step)\n",
    "        critic_fake_image = critic(fake_image.detach(), alpha, step)\n",
    "        gp = gradient_penalty(critic, image, fake_image, alpha, step)\n",
    "        loss_critic = (\n",
    "            -(torch.mean(critic_image) - torch.mean(critic_fake_image))\n",
    "            + LAMBDA_GP * gp\n",
    "            + (0.001 * torch.mean(critic_image ** 2))\n",
    "        )\n",
    "\n",
    "        opt_critic.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        opt_critic.step()\n",
    "\n",
    "        # Train Generator: max E[critic(gen_fake_image)] <-> min -E[critic(gen_fake_image)]\n",
    "        gen_fake_image = critic(fake_image, alpha, step)\n",
    "        loss_gen = -torch.mean(gen_fake_image)\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        loss_gen.backward(retain_graph=True)\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Update alpha and ensure less than 1\n",
    "        alpha += cur_batch_size / (\n",
    "            (PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n",
    "        )\n",
    "        alpha = min(alpha, 1)\n",
    "\n",
    "        if batch_idx % 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                fixed_fake_images = gen(FIXED_NOISE, alpha, step) * 0.5 + 0.5\n",
    "            plot_to_tensorboard(\n",
    "                writer,\n",
    "                loss_critic.item(),\n",
    "                loss_gen.item(),\n",
    "                image.detach(),\n",
    "                fixed_fake_images.detach(),\n",
    "                tensorboard_step,\n",
    "            )\n",
    "            tensorboard_step += 1\n",
    "\n",
    "        loop.set_postfix(\n",
    "            gp=gp.item(),\n",
    "            loss_critic=loss_critic.item(),\n",
    "        )\n",
    "\n",
    "    return tensorboard_step, alpha\n",
    "\n",
    "\n",
    "def main():\n",
    "    gen = Generator(\n",
    "        latent_space, in_filters, img_channels\n",
    "    )\n",
    "    critic = Discriminator(\n",
    "        in_filters, img_channels\n",
    "    )\n",
    "\n",
    "    # initialize optimizers and scalers for FP16 training\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
    "    opt_critic = optim.Adam(\n",
    "        critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99)\n",
    "    )\n",
    "\n",
    "    # for tensorboard plotting\n",
    "    writer = SummaryWriter(f\"logs/gan1\")\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_CRITIC, critic, opt_critic, LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "    gen.train()\n",
    "    critic.train()\n",
    "\n",
    "    tensorboard_step = 0\n",
    "    # start at step that corresponds to img size that we set in config\n",
    "    step = int(log2(START_TRAIN_AT_IMG_SIZE / 4))\n",
    "    for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "        alpha = 1e-5  # start with very low alpha\n",
    "        loader, dataset = get_loader(4 * 2 ** step)  # 4->0, 8->1, 16->2, 32->3, 64 -> 4\n",
    "        print(f\"Current image size: {4 * 2 ** step}\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            tensorboard_step, alpha = train_fn(\n",
    "                critic,\n",
    "                gen,\n",
    "                loader,\n",
    "                dataset,\n",
    "                step,\n",
    "                alpha,\n",
    "                opt_critic,\n",
    "                opt_gen,\n",
    "                tensorboard_step,\n",
    "                writer\n",
    "            )\n",
    "\n",
    "            if SAVE_MODEL:\n",
    "                save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
    "                save_checkpoint(critic, opt_critic, filename=CHECKPOINT_CRITIC)\n",
    "\n",
    "        step += 1  # progress to the next img size\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8fac24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".joker",
   "language": "python",
   "name": ".joker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
