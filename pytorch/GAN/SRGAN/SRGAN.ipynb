{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e6e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bfd2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        channels,\n",
    "        out_channels,\n",
    "        discriminator=False, \n",
    "        use_act=True, \n",
    "        use_norm=True, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_act = use_act\n",
    "        self.conv = nn.Conv2d(channels, out_channels, **kwargs, bias=not use_norm)\n",
    "        self.norm = nn.InstanceNorm2d(out_channels,affine=True) if use_norm else nn.Identity()\n",
    "        self.act = (\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "            if discriminator\n",
    "            else nn.PReLU(num_parameters=out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.act(self.norm(self.conv(x))) if self.use_act else self.norm(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba6ab2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        nn.Block1 = ConvBlock(\n",
    "            channels,\n",
    "            channels,\n",
    "            kernel_size = 3,\n",
    "            stride = 1,\n",
    "            padding = 1\n",
    "        )\n",
    "        \n",
    "        nn.Block2 = ConvBlock(\n",
    "            channels,\n",
    "            channels,\n",
    "            kernel_size = 3,\n",
    "            stride = 1,\n",
    "            padding = 1,\n",
    "            use_act=False\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = nn.Block1(x)\n",
    "        out = nn.Block2(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28a77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelShuffler(nn.Module):\n",
    "    def __init__(self, channels, scale_factor):\n",
    "        super().__init__()\n",
    "        self.conv = ConvBlock(channels,\n",
    "                              channels*scale_factor ** 2,\n",
    "                              kernel_size= 3,\n",
    "                              stride= 1,\n",
    "                              padding= 1)\n",
    "        self.ps = nn.PixelShuffle(scale_factor) #c*4,h,w -> c,h*2,w*2\n",
    "        self.act = nn.PReLU(channels)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.act(self.ps(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6af7f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels=3, out_channels=64, num_blocks=16):\n",
    "        super().__init__()\n",
    "        self.initial = ConvBlock(channels, out_channels, kernel_size=9, stride=1, padding=4, use_norm=False)\n",
    "        self.residuals = nn.Sequential(*[ResidualBlock(out_channels) for _ in range(num_blocks)])\n",
    "        self.convblock = ConvBlock(out_channels, out_channels, kernel_size=3, stride=1, padding=1, use_act=False)\n",
    "        self.upsamples = nn.Sequential(PixelShuffler(out_channels, 2), PixelShuffler(out_channels, 2))\n",
    "        self.final = nn.Conv2d(out_channels, channels, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        initial = self.initial(x)\n",
    "        x = self.residuals(initial)\n",
    "        x = self.convblock(x) + initial\n",
    "        x = self.upsamples(x)\n",
    "        return torch.tanh(self.final(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e70571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=3, features=[64, 64, 128, 128, 256, 256, 512, 512]):\n",
    "        super().__init__()\n",
    "        blocks = []\n",
    "        for idx, feature in enumerate(features):\n",
    "            blocks.append(\n",
    "                ConvBlock(\n",
    "                    channels,\n",
    "                    feature,\n",
    "                    kernel_size=3,\n",
    "                    stride=1 + idx % 2,\n",
    "                    padding=1,\n",
    "                    discriminator=True,\n",
    "                    use_act=True,\n",
    "                    use_norm=False if idx == 0 else True,\n",
    "                )\n",
    "            )\n",
    "            channels = feature\n",
    "\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((6, 6)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512*6*6, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c1e67ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test():\n",
    "#     low_resolution = 24  # 96x96 -> 24x24\n",
    "    \n",
    "#     x = torch.randn((5, 3, low_resolution, low_resolution))\n",
    "#     gen = Generator()\n",
    "#     gen_out = gen(x)\n",
    "#     disc = Discriminator()\n",
    "#     disc_out = disc(gen_out)\n",
    "\n",
    "#     print(gen_out.shape)\n",
    "#     print(disc_out.shape)\n",
    "\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4d3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg19\n",
    "\n",
    "# phi_5,4 5th conv layer before maxpooling but after activation\n",
    "\n",
    "class VGGloss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vgg = vgg19(weights=True).features[:36].eval()\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        vgg_input_features = self.vgg(inputs)\n",
    "        vgg_target_features = self.vgg(target)\n",
    "        return self.loss(vgg_input_features, vgg_target_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04f082b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_GEN = \"gen.pth.tar\"\n",
    "CHECKPOINT_DISC = \"disc.pth.tar\"\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "HIGH_RES = 96\n",
    "LOW_RES = HIGH_RES // 4\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "highres_transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "lowres_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(width=LOW_RES, height=LOW_RES, interpolation=Image.BICUBIC),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "both_transforms = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(width=HIGH_RES, height=HIGH_RES),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e900476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyImageFolder(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        super(MyImageFolder, self).__init__()\n",
    "        self.data = []\n",
    "        self.root_dir = root_dir\n",
    "        self.class_names = os.listdir(root_dir)\n",
    "\n",
    "        for index, name in enumerate(self.class_names):\n",
    "            files = os.listdir(os.path.join(root_dir, name))\n",
    "            self.data += list(zip(files, [index] * len(files)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_file, label = self.data[index]\n",
    "        root_and_dir = os.path.join(self.root_dir, self.class_names[label])\n",
    "\n",
    "        image = np.array(Image.open(os.path.join(root_and_dir, img_file)))\n",
    "        image = both_transforms(image=image)[\"image\"]\n",
    "        high_res = highres_transform(image=image)[\"image\"]\n",
    "        low_res = lowres_transform(image=image)[\"image\"]\n",
    "        return low_res, high_res\n",
    "\n",
    "\n",
    "# def test():\n",
    "#     dataset = MyImageFolder(root_dir=\"faces/\")\n",
    "#     loader = DataLoader(dataset, batch_size=1, num_workers=8)\n",
    "\n",
    "#     for low_res, high_res in loader:\n",
    "#         print(low_res.shape)\n",
    "#         print(high_res.shape)\n",
    "\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64b974f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, device):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * alpha + fake.detach() * (1 - alpha)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=config.DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "\n",
    "def plot_examples(low_res_folder, gen):\n",
    "    files = os.listdir(low_res_folder)\n",
    "\n",
    "    gen.eval()\n",
    "    for file in files:\n",
    "        image = Image.open(\"test_images/\" + file)\n",
    "        with torch.no_grad():\n",
    "            upscaled_img = gen(\n",
    "                config.test_transform(image=np.asarray(image))[\"image\"]\n",
    "                .unsqueeze(0)\n",
    "                .to(config.DEVICE)\n",
    "            )\n",
    "        save_image(upscaled_img * 0.5 + 0.5, f\"saved/{file}\")\n",
    "    gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94e889ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_fn(loader, disc, gen, opt_gen, opt_disc, mse, bce, vgg_loss):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for idx, (low_res, high_res) in enumerate(loop):\n",
    "        \n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        fake = gen(low_res)\n",
    "        disc_real = disc(high_res)\n",
    "        disc_fake = disc(fake.detach())\n",
    "        disc_loss_real = bce(\n",
    "            disc_real, torch.ones_like(disc_real) - 0.1 * torch.rand_like(disc_real)\n",
    "        )\n",
    "        disc_loss_fake = bce(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = disc_loss_fake + disc_loss_real\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        # Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        disc_fake = disc(fake)\n",
    "        #l2_loss = mse(fake, high_res)\n",
    "        adversarial_loss = 1e-3 * bce(disc_fake, torch.ones_like(disc_fake)) \n",
    "        loss_for_vgg = 0.006 * vgg_loss(fake, high_res)\n",
    "        gen_loss = loss_for_vgg + adversarial_loss\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if idx % 200 == 0:\n",
    "            plot_examples(\"test_images/\", gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zok/joker/.joker/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  8%|███▌                                        | 2/25 [00:20<03:45,  9.79s/it]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    dataset = MyImageFolder(root_dir=\"faces/\")\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "    )\n",
    "    gen = Generator()\n",
    "    disc = Discriminator()\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
    "    opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
    "    mse = nn.MSELoss()\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    vgg_loss = VGGloss()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_GEN,\n",
    "            gen,\n",
    "            opt_gen,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "           CHECKPOINT_DISC, disc, opt_disc, LEARNING_RATE,\n",
    "        )\n",
    "        \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_fn(loader, disc, gen, opt_gen, opt_disc, mse, bce, vgg_loss)\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
    "            save_checkpoint(disc, opt_disc, filename=CHECKPOINT_DISC)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a10d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".joker",
   "language": "python",
   "name": ".joker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
